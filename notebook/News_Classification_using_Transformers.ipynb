{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Classification using Transformers",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_LAdk1TpgqV",
        "outputId": "61545e53-812e-457d-fb82-364d2c98de02"
      },
      "source": [
        "!git clone https://github.com/Josepholaidepetro/Docict"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Docict'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1O9fFe9qMua"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAAVgOxOo9tg"
      },
      "source": [
        "!unzip \"/content/Docict/data/bbc data.zip\" -d \"/content/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXTZrwO9sKOP"
      },
      "source": [
        "import glob\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAYG-dhFppyr"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBa2caVZtF9v"
      },
      "source": [
        "data = []\n",
        "for filename in glob.glob(f'/content/data/bbc-fulltext (document classification)/*/*/*.txt'):\n",
        "  data.append(filename)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CdnApT9tHHY",
        "outputId": "da9e629f-218e-4fbf-d158-b175de82bbb9"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2225"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6vDCWxFuWt4"
      },
      "source": [
        "docs = []\n",
        "for i in data:\n",
        "  label = i.split('/')[-2]\n",
        "  # {'business', 'entertainment', 'politics', 'sport', 'tech'}\n",
        "  if label == 'business':\n",
        "    label = 0\n",
        "  elif label == 'entertainment':\n",
        "    label = 1\n",
        "  elif label == 'politics':\n",
        "    label = 2\n",
        "  elif label == 'sport':\n",
        "    label = 3\n",
        "  else:\n",
        "    label = 4\n",
        "  with open(i, 'r') as f:\n",
        "    try:\n",
        "      dataop = f.read()\n",
        "      docs.append((dataop, label))\n",
        "    except:\n",
        "      pass "
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFtiYNoQ_azY",
        "outputId": "a36a6735-c76d-494e-edf9-894b0c1d7701"
      },
      "source": [
        "np.random.shuffle(docs)\n",
        "num_val_samples = int(0.2 * len(docs))\n",
        "num_train_samples = len(docs) - 2 * num_val_samples\n",
        "train_pairs = docs[:num_train_samples]\n",
        "val_pairs = docs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = docs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(docs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2224 total pairs\n",
            "1336 training pairs\n",
            "444 validation pairs\n",
            "444 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wslSNqqF7aYP"
      },
      "source": [
        "vocab_size = 25000\n",
        "sequence_length = 200\n",
        "batch_size = 64"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ4i1nrPw11-"
      },
      "source": [
        "def tf_clean(text):\n",
        "  text = tf.strings.regex_replace(text, '\\n', '')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', '')\n",
        "  return text\n",
        "\n",
        "train_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "    standardize=tf_clean,\n",
        ")\n",
        "train_texts = [pair[0] for pair in docs]\n",
        "train_vectorization.adapt(train_texts)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWJuffpXAWEe"
      },
      "source": [
        "def format_dataset(doc, label):\n",
        "    vec = train_vectorization(doc)\n",
        "    return vec, label"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfY7F3YL7RZk"
      },
      "source": [
        "def make_dataset(pairs):\n",
        "    texts, labels = zip(*pairs)\n",
        "    texts = list(texts)\n",
        "    labels = list(labels)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjTKRtOKNP10"
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h758HI7pSqQQ"
      },
      "source": [
        "# Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7AofWWeC2J4"
      },
      "source": [
        "class Transformerlayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(Transformerlayer, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDkjCdc9SvUW"
      },
      "source": [
        "# Positional Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l11a_U-dEW5Q"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "        \n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUdUlzJYSz33"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2xhKzyCEsvm"
      },
      "source": [
        "embed_dim = 64\n",
        "latent_dim = 128\n",
        "num_heads = 4\n",
        "\n",
        "transformer_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"transformer_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(transformer_inputs)\n",
        "transformer_outputs = Transformerlayer(embed_dim, latent_dim, num_heads)(x)\n",
        "x = layers.GlobalAveragePooling1D()(transformer_outputs)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xln2E7IWGwJ9"
      },
      "source": [
        "model = keras.Model(inputs=transformer_inputs, outputs=outputs)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRgJrsOtINEZ",
        "outputId": "11fefd3f-acad-4b16-94f3-304c069cd5a5"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    train_ds, epochs=20, validation_data=val_ds\n",
        ")"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - 17s 702ms/step - loss: 1.5231 - accuracy: 0.3630 - val_loss: 1.2919 - val_accuracy: 0.4730\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 14s 664ms/step - loss: 0.7944 - accuracy: 0.7994 - val_loss: 0.3720 - val_accuracy: 0.9257\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 14s 669ms/step - loss: 0.1185 - accuracy: 0.9858 - val_loss: 0.1462 - val_accuracy: 0.9505\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 14s 668ms/step - loss: 0.0220 - accuracy: 0.9985 - val_loss: 0.1507 - val_accuracy: 0.9617\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 14s 669ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1283 - val_accuracy: 0.9730\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 14s 663ms/step - loss: 0.0068 - accuracy: 0.9993 - val_loss: 0.1441 - val_accuracy: 0.9595\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 14s 659ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1472 - val_accuracy: 0.9617\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 14s 658ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9617\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 14s 660ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9662\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 15s 696ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9707\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 15s 711ms/step - loss: 6.6176e-04 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9707\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 15s 701ms/step - loss: 7.5353e-04 - accuracy: 1.0000 - val_loss: 0.1336 - val_accuracy: 0.9707\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 15s 704ms/step - loss: 5.8299e-04 - accuracy: 1.0000 - val_loss: 0.1372 - val_accuracy: 0.9685\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 15s 713ms/step - loss: 5.8237e-04 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 0.9707\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 15s 704ms/step - loss: 5.5549e-04 - accuracy: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.9707\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 15s 707ms/step - loss: 4.8856e-04 - accuracy: 1.0000 - val_loss: 0.1400 - val_accuracy: 0.9707\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 15s 713ms/step - loss: 4.3078e-04 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 0.9707\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 15s 702ms/step - loss: 3.9895e-04 - accuracy: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.9707\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 15s 705ms/step - loss: 3.8993e-04 - accuracy: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.9707\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 15s 715ms/step - loss: 3.5049e-04 - accuracy: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.9707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy6fdNqGO5iV"
      },
      "source": [
        "test_ds = make_dataset(test_pairs)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJYAxB15IaW9",
        "outputId": "16221e49-ce25-4751-c53c-f333a0a46daf"
      },
      "source": [
        "model.evaluate(test_ds)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 2s 251ms/step - loss: 0.1188 - accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11882788687944412, 0.9729729890823364]"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    }
  ]
}
